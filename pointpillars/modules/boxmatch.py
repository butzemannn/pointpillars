# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_modules.boxmatch.ipynb (unless otherwise specified).

__all__ = ['logger', 'BoxMatch']

# Cell
import logging
import ast
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union
from math import pi
from torchvision.ops import box_iou

from utils.math.box_ops import convert_boxes_to_2d_corners

logger = logging.getLogger(__name__)

# Cell
class BoxMatch(nn.Module):
    def __init__(self, pillars_cfg, head_cfg, training: bool = True):

        super(BoxMatch, self).__init__()
        self.pillars_cfg = pillars_cfg
        self.head_cfg = head_cfg
        self.anchors = ast.literal_eval(head_cfg['anchors'])
        self.training = training

        self.x_min =  self.pillars_cfg.getfloat('x_min')
        self.y_min =  self.pillars_cfg.getfloat('y_min')


    def _calculate_absolute_boxes_from_anchors(self, pred_anchors: torch.Tensor) -> torch.Tensor:
        """Returns a view of all predicted, absolute bounding boxes.

        :param pred_anchors: tensor(batch_size, img_leng, img_with, nbr_anchors, nbr_attributes=7)

        :returns: tensor(batch_size, img_leng, img_width, nbr_anchors, nbr_attributes=7)
        """
        bs, n_x, n_y, n_a = pred_anchors.shape[0:4]
        x_step = (self.pillars_cfg.getfloat('x_max') - self.x_min) / n_x
        y_step = (self.pillars_cfg.getfloat('y_max') - self.y_min) / n_y
        # create tensor containing the pillar indicies in the correct dimension
        x = torch.tensor(range(n_x), device=torch.device("cuda")).unsqueeze(1).expand(n_x, n_y)
        y = torch.tensor(range(n_y), device=torch.device("cuda")).unsqueeze(0).expand(n_x, n_y)
        ind = torch.stack((x,y), dim=2).unsqueeze(0).expand(bs, -1, -1, -1)

        # add anchors to receive other absolute values
        anchors_tens = torch.tensor(self.anchors, device=torch.device("cuda"))
        anchors_tens = anchors_tens.unsqueeze(0).unsqueeze(0).unsqueeze(0).expand(bs, n_x, n_y, -1, -1)

        # step with current pseudo image size
        # calculate pillar center from index
        pil_xy = ind * torch.tensor([x_step, y_step], device=torch.device("cuda"))
        pil_xy += torch.tensor([self.x_min, self.y_min], device=torch.device("cuda"))

        # expand for anchor nbr
        pil_xy = pil_xy.unsqueeze(3).expand(-1, -1, -1, n_a, -1)

        diag = torch.sqrt(torch.pow(pred_anchors[:,:,:,:,5], 2) + torch.pow(pred_anchors[:,:,:,:,4], 2))
        diag = diag.unsqueeze(4).expand(-1, -1, -1, -1, 2)
        # add center to offset
        pred_anchors_xy = pred_anchors[:,:,:,:,:2] * diag + pil_xy

        # add anchors z offset
        pred_anchors_z = pred_anchors[:,:,:,:,2] * pred_anchors[:,:,:,:,3] + anchors_tens[:,:,:,:,0]
        pred_anchors_z = pred_anchors_z.unsqueeze(4)
        pred_anchors_hwl = torch.exp(pred_anchors[:,:,:,:,3:6]) * anchors_tens[:,:,:,:,1:4]

        # add radiant offset
        epsilon = 1e-7
        pred_anchors_theta = torch.clamp(pred_anchors[:,:,:,:,6], -1 + epsilon, +1 - epsilon)
        pred_anchors_theta = -1 * torch.arcsin(pred_anchors_theta) + anchors_tens[:,:,:,:,4]
        pred_anchors_theta = pred_anchors_theta.unsqueeze(4)

        pred_anchors = torch.cat((pred_anchors_xy, pred_anchors_z, pred_anchors_hwl, pred_anchors_theta), dim=4)

        return pred_anchors


    def _calculate_batch_iou(self, pred_boxes: torch.Tensor, gt_boxes: torch.Tensor) -> torch.Tensor:
        """Calculates the iou for batched prediction and ground truth boxes.

        :param pred_boxes: Tensor(batch_size, nbr_pred_boxes, 4)
                            with last dimension being (x_min, y_min, x_max, y_max)
        :param gt_boxes: Tensor(batch_size, nbr_gt_boxes, 4)
                            with last dimension being (x_min, y_min, x_max, y_max)

        :returns: Tensor(batch_size, nbr_pred_boxes, nbr_gt_boxes)
        """
        iou =  []
        for i in range(pred_boxes.shape[0]):
            pred_batch = pred_boxes[i]
            gt_batch = gt_boxes[i]
            iou.append(box_iou(pred_batch, gt_batch))

        batch_iou = torch.stack(iou, dim=0)
        return batch_iou

    def boxmatch_from_corners(self,
                              preds: Union[list, tuple],
                              gt_boxes: torch.Tensor,
                              gt_mask: torch.Tensor,
                              pos_iou_threshold: float = 0.6,
                              neg_iou_threshold: float = 0.4) -> list:
        """
        Calculates iou from boxes in corner representation and uses non maximum suppression to get
        the best fitting boxes.

        :param preds: A tuple containing all predicted values for the batch. Must have the shape:
                      (pred_occ(N, H * W * nb_anchors),
                       pred_cls(N, H * W * nb_anchors),
                       pred_head(N, H * W * nb_anchors),
                       pred_box(N, H * W * nb_anchors, nb_attributes=7))

                       pred_box having attributes x,y,z,h,w,l,theta

        :param gt_boxes: Tensor(batch_size, nbr_gt_boxes, nbr_attributes=7)
        :param gt_mask: tensor(batch_size, nbr_gt_boxes)
                        Containing 1 or 0, depending if gt_box will be used for loss calculation
        :param pos_iou_threshold: float to give the upper threshold for positive box matching
        :param neg_iou_threshold: float to give the lower threshold for negative box matching
                                    only used to learn occupancy parameter

        :returns: A list containing the matched tensors. The list has the following structure:
                  [pred_occ(nb_matched_boxes),
                   pred_cls(nb_matched_boxes),
                   pred_head(nb_matched_boxes),
                   pred_boxes(nb_matched_boxes, nb_attributes=7),
                   gt_boxes(nb_matched_boxes, nb_attributes=7),
                   neg_matches(nb_negative_match)
                   gt_mask(nb_matched_boxes)]

                   neg_match gives the pred_occ for every box with a iou match lower than the neg_iou_threshold
        """

        if isinstance(preds, tuple):
            preds = list(preds)

        pred_boxes_corners = convert_boxes_to_2d_corners(preds[3])
        gt_boxes_corners = convert_boxes_to_2d_corners(gt_boxes)
        # upper iou threshold for positive as well as negative match
        # add x_min and y_min, since box_iou() expects values greater than 0
        bs, na, n = pred_boxes_corners.shape
        xy_min = torch.tensor([self.x_min, self.y_min], device=torch.device("cuda"))
        xy_min = torch.cat((xy_min, xy_min), dim=0)
        iou = self._calculate_batch_iou(pred_boxes_corners + xy_min.expand_as(pred_boxes_corners),
                                        gt_boxes_corners + xy_min.expand_as(gt_boxes_corners))

        condition_pos = iou >= pos_iou_threshold
        # incorporate gt_mask
        gt_mask_pos = gt_mask.unsqueeze(1).expand(-1, condition_pos.shape[1], -1)
        condition_pos = torch.logical_and(condition_pos.cuda(), gt_mask_pos)
        indices_pos = torch.nonzero(condition_pos, as_tuple=False)

        # batch_size * nbr_pred_boxes has to be added to index since pred_boxes is flattened
        indices_pred_boxes_flat = indices_pos[:,1] + indices_pos[:,0] * preds[3].shape[1]
        indices_gt_boxes_flat = indices_pos[:,2] + indices_pos[:,0] * gt_boxes.shape[1]

        results = []
        for tensor in preds:
            # flatten since batch dimension is no longer necessary after matching
            flat_tensor = torch.flatten(tensor, start_dim=0, end_dim=1)
            # select indices according to iou
            results.append(torch.index_select(flat_tensor, 0, indices_pred_boxes_flat))
        gt_boxes_flat = torch.flatten(gt_boxes, start_dim=0, end_dim=1)
        results.append(torch.index_select(gt_boxes_flat, 0, indices_gt_boxes_flat))

        # negative matches
        # same index procedure as previously done
        condition_neg = iou <= neg_iou_threshold
        gt_mask_neg = gt_mask.unsqueeze(1).expand(-1, condition_neg.shape[1], -1)
        condition_neg = torch.logical_and(condition_neg, gt_mask_neg)
        indices_neg = torch.nonzero(condition_neg, as_tuple=False)
        indices_neg_flat = indices_neg[:,1] + indices_neg[:,0] * preds[3].shape[1]

        pred_occ_flat = torch.flatten(preds[0], start_dim=0, end_dim=1)
        results.append(torch.index_select(pred_occ_flat, 0, indices_neg_flat))

        return results

    def forward(self,
                preds: Union[list, tuple],
                gt_boxes: torch.Tensor,
                gt_mask: torch.Tensor) -> list:
        """Matches anchor predictions to gt_boxes with IoU

        :param preds: A tuple containing the predictions from the net. Must have the following shape:
                      (pred_occ(N, H, W, nb_anchors),
                       pred_cls(N, H, W, nb_anchors),
                       pred_head(N, H, W, nb_anchors),
                       pred_box(N, H, W, nb_anchors, nb_attributes=7))

        :param gt_boxes: tensor(batch_size, gt_boxes per image, nbr_attributes)
        :param gt_mask: tensor(batch_size, gt_boxes)
                        Containing 1 or 0, depending if gt_box will be used for loss calculation

        :returns: A list containing the matched tensors. The list has the following structure:
                  [pred_occ(nb_matched_boxes),
                   pred_cls(nb_matched_boxes),
                   pred_head(nb_matched_boxes),
                   pred_boxes(nb_matched_boxes, nb_attributes=7),
                   gt_boxes(nb_matched_boxes, nb_attributes=7),
                   neg_matches(nb_negative_match)
                   gt_mask(nb_matched_boxes)]

                   neg_match gives the pred_occ for every box with a iou match lower than the neg_iou_threshold
        """

        if isinstance(preds, tuple):
            preds = list(preds)
        # calc anchor centers from pseudo image index
        preds[3] = self._calculate_absolute_boxes_from_anchors(preds[3])
        # shape Tensor(batch_size, H * W * nbr_anchors, nbr_attributes=7)
        for i in range(len(preds)):
            preds[i] = torch.flatten(preds[i], start_dim=1, end_dim=3)

        if self.training:
            return self.boxmatch_from_corners(preds,
                                              gt_boxes,
                                              gt_mask,
                                              pos_iou_threshold=self.pillars_cfg.getfloat('pos_iou_threshold'),
                                              neg_iou_threshold=self.pillars_cfg.getfloat('neg_iou_threshold'))
        else:
            return preds
