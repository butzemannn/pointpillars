{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"/home/qhs67/git/bachelorthesis_sven_thaele/code/pointpillars/\")\n",
    "sys.path.insert(0, \"../pointpillars\")\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from torchviz import make_dot\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from utils.io import read_config, save_network_checkpoint, save_network\n",
    "from data.dataset import VelTrainDataset, collate_fn, OverfitSampler, collate_wrapper\n",
    "from modules.pointpillars import PointPillars, init_weights\n",
    "from modules.loss import PointPillarsLoss\n",
    "\n",
    "\n",
    "#import os\n",
    "#os.environ['PATH'] += os.pathsep + \"/home/qhs67/anaconda3/envs/ba/lib/python3.8/site-packages/graphviz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# time to identify the run on folders and checkpoints etc\n",
    "ident_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "log_formatter = logging.Formatter(log_format)\n",
    "\n",
    "log_handler = logging.FileHandler(\"/home/qhs67/git/bachelorthesis_sven_thaele/code/pointpillars.log\", mode='w')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(log_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# tensorboard writer\n",
    "run_folder = \"/home/qhs67/git/bachelorthesis_sven_thaele/code/runs/{}/\".format(ident_time)\n",
    "\n",
    "writer = SummaryWriter(run_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensor([[0.9286, 0.5153, 0.1014, 0.4156, 0.7509, 0.6023, 0.1398, 0.5218, 0.9959,\\n         0.1809],\\n        [0.7044, 0.5028, 0.0514, 0.3307, 0.8468, 0.4850, 0.0378, 0.3771, 0.6260,\\n         0.6868],\\n        [0.8171, 0.3611, 0.0451, 0.8152, 0.4421, 0.7532, 0.9213, 0.5106, 0.5432,\\n         0.1461],\\n        [0.2137, 0.6373, 0.2152, 0.6954, 0.9749, 0.8745, 0.4388, 0.8018, 0.7215,\\n         0.3250],\\n        [0.2791, 0.6374, 0.5947, 0.2953, 0.0867, 0.7234, 0.8654, 0.3448, 0.2403,\\n         0.2191],\\n        [0.2610, 0.9185, 0.9933, 0.1652, 0.8397, 0.6247, 0.3722, 0.2197, 0.1521,\\n         0.4104],\\n        [0.0108, 0.1410, 0.5190, 0.0044, 0.1795, 0.9898, 0.4361, 0.3983, 0.7308,\\n         0.2474],\\n        [0.0116, 0.4330, 0.9442, 0.3807, 0.5948, 0.3696, 0.3961, 0.3705, 0.9404,\\n         0.3496],\\n        [0.5895, 0.4239, 0.3905, 0.4346, 0.2233, 0.3486, 0.2616, 0.8417, 0.8531,\\n         0.6779],\\n        [0.5821, 0.8227, 0.6498, 0.4859, 0.8083, 0.3821, 0.1802, 0.5034, 0.9351,\\n         0.3870]])'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.rand((10,10))\n",
    "tens.__str__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def _train_setup():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size = 3\n",
    "    init_lr = 2 * 10**-4\n",
    "    #init_lr = 1 * 10**-4\n",
    "\n",
    "    logger.info(\"Start network training..\")\n",
    "    torch.cuda.empty_cache()\n",
    "    #torch.multiprocessing.set_start_method('spawn')\n",
    "    # TODO: move to config file\n",
    "    conf = read_config()\n",
    "    vel_folder = \"/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/velodyne/training\"\n",
    "    label_folder =\"/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/label_2/training\"\n",
    "    ds_train = VelTrainDataset(vel_folder, label_folder)\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           num_workers=2,\n",
    "                                           collate_fn=collate_wrapper,\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=True)\n",
    "    \"\"\"sampler = OverfitSampler(ds_train, batch_size, nb_samples=20, shuffle=True)\n",
    "    for i in sampler:\n",
    "        print(i)\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=sampler,\n",
    "                                           num_workers=0,\n",
    "                                           collate_fn=collate_fn)\"\"\"\n",
    "    # modules\n",
    "    pointpillars = PointPillars(conf)\n",
    "    loss_func = PointPillarsLoss()\n",
    "    pointpillars.train()\n",
    "    loss_func.train()\n",
    "\n",
    "    # TODO: also init bias?\n",
    "    #pointpillars.apply(init_weights)\n",
    "    # move to gpu\n",
    "    pointpillars.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(pointpillars.parameters(), lr=init_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.8, last_epoch=-1)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2000, gamma=0.8, last_epoch=-1)\n",
    "\n",
    "    return pointpillars, loss_func, optimizer, scheduler, dl_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def _train_step(batch: torch.Tensor,\n",
    "               pointpillars: torch.nn.Module,\n",
    "               loss_func: torch.nn.Module,\n",
    "               optimizer: torch.optim.Adam,\n",
    "               epoch: int,\n",
    "               i: int) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Performs a training step\n",
    "    \"\"\"\n",
    "    pil_batch, ind_batch, label_batch, label_mask = batch\n",
    "\n",
    "    pil_batch = pil_batch.cuda(non_blocking=True)\n",
    "    ind_batch = ind_batch.cuda(non_blocking=True)\n",
    "    label_batch = label_batch.cuda(non_blocking=True)\n",
    "    label_mask = label_mask.cuda(non_blocking=True)\n",
    "    # -> forward pass through network\n",
    "\n",
    "    preds = pointpillars(pil_batch, ind_batch, label_batch, label_mask)\n",
    "    loss = loss_func(preds, writer, epoch, i)\n",
    "\n",
    "    \"\"\"dot = make_dot(loss, params=dict(pointpillars.named_parameters()), show_saved=True, show_attrs=True)\n",
    "    dot.format = 'png'\n",
    "    dot.render('torchviz-sample')\"\"\"\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del pil_batch, ind_batch, label_batch, preds\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def validate(network: torch.nn.Module, loss_func: torch.nn.Module, epoch, nbr_val_batches: int = 300):\n",
    "    \"\"\"\n",
    "    Validates the network on the loss function on the validation dataset\n",
    "    \"\"\"\n",
    "    batch_size = 3\n",
    "    validation_folder = \"/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/velodyne/validation\"\n",
    "    label_folder =\"/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/label_2/validation\"\n",
    "    with torch.no_grad():\n",
    "        ds_val = VelTrainDataset(validation_folder, label_folder)\n",
    "        dl_val = torch.utils.data.DataLoader(ds_val,\n",
    "                                           batch_size=batch_size,\n",
    "                                           num_workers=2,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=True)\n",
    "\n",
    "        running_val_loss = 0\n",
    "        for i, batch in enumerate(dl_val):\n",
    "            # stop after given number of data\n",
    "            if i >= nbr_val_batches:\n",
    "                break\n",
    "            pil_batch, ind_batch, label_batch, label_mask = batch\n",
    "            preds = network(pil_batch, ind_batch, label_batch, label_mask)\n",
    "            loss = loss_func(preds)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            print(\"Val Epoch: {}, Batch {} with Loss {} and running Loss {}.\".format(epoch, i, loss.item(), running_val_loss/(i+1)))\n",
    "            #writer.add_scalar(\"Epoch {}/Validation Loss\".format(epoch), running_val_loss/(i+1), i)\n",
    "            #writer.flush()\n",
    "\n",
    "        writer.add_scalar(\"Epochs/Validation Loss\", running_val_loss/nbr_val_batches, epoch)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def train(val: bool = False, save_nw: bool = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param val: bool if validation should be used\n",
    "    :param save_nw: bool if network state should be saved after training\n",
    "    \"\"\"\n",
    "    n_epochs = 1\n",
    "\n",
    "    pointpillars, loss_func, optimizer, scheduler, dl_train = _train_setup()\n",
    "\n",
    "\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            running_loss = 0\n",
    "            with torch.profiler.profile(\n",
    "                schedule=torch.profiler.schedule(wait=0, warmup=1, active=3, repeat=2),\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/prof1'),\n",
    "                record_shapes=True,\n",
    "                with_stack=True,\n",
    "\n",
    "            ) as prof:\n",
    "                for i, batch in enumerate(dl_train):\n",
    "                    if i >= (1 + 3) * 2:\n",
    "                        break\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = _train_step(batch, pointpillars, loss_func, optimizer, epoch, i)\n",
    "                    #running_loss += loss.item()\n",
    "                    #print(\"Epoch: {}, Batch {} with Loss {} and running Loss {}.\".format(epoch, i, loss.item(), running_loss/(i+1)))\n",
    "                    print(\"Epoch: {}, Batch {}.\".format(epoch, i))\n",
    "                    #writer.add_scalar(\"Epoch {}/Running Loss\".format(epoch), running_loss/(i+1), i)\n",
    "                    #writer.flush()\n",
    "                    prof.step()\n",
    "\n",
    "                # after epoch\n",
    "                #scheduler.step()\n",
    "                #writer.add_scalar(\"Epochs/Running Loss\", running_loss/len(dl_train), epoch)\n",
    "                #writer.add_scalar(\"Epochs/Learning Rate\", scheduler.get_last_lr()[0], epoch)\n",
    "                #writer.flush()\n",
    "            #if val:\n",
    "                #validate(pointpillars, loss_func, epoch, nbr_val_batches=100)\n",
    "        # after training\n",
    "        print('Finished Training')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logger.exception(\"An exception occured\")\n",
    "        save_network_checkpoint(pointpillars, optimizer, scheduler, loss, ident_time, epoch)\n",
    "        exit()\n",
    "    if save_nw:\n",
    "        # save network to location from config\n",
    "        save_network(pointpillars, ident_time, n_epochs)\n",
    "\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[   0,    1,    2],\n",
       "          [  10,   20,   30]],\n",
       " \n",
       "         [[ 100,  200,  300],\n",
       "          [1000, 2000, 3000]]]),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[0, 1, 2], [10, 20, 30]], [[100, 200, 300], [1000, 2000, 3000]]])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    1,    2],\n",
       "         [  10,   20,   30],\n",
       "         [ 100,  200,  300],\n",
       "         [1000, 2000, 3000]]),\n",
       " torch.Size([4, 3]),\n",
       " tensor([100, 200, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.flatten(a, start_dim=0, end_dim=1)\n",
    "b, b.shape, b[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qhs67/anaconda3/envs/ba/lib/python3.8/site-packages/torch/cuda/memory.py:373: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_cached())\n",
    "#train(val=True, save_nw=False)\n",
    "#make_dot(loss, params=dict(pointpillars.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch 0 with Loss 2.6619033813476562 and running Loss 2.6619033813476562.\n",
      "Epoch: 0, Batch 1 with Loss 2.680222988128662 and running Loss 2.671063184738159.\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_interference\"):\n",
    "        train(save_nw=False)\n",
    "        \n",
    "    print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "if __name__ == '__main__':\n",
    "    train(save_nw=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba",
   "language": "python",
   "name": "ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}